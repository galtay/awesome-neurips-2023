{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of NeurIPS 2023 Accepted Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc_file('matplotlibrc')\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/neurips2022data.json\", \"r\") as f:\n",
    "    neurips_2022_data = json.load(f)\n",
    "\n",
    "with open(\"data/neurips2023data.json\", \"r\") as f:\n",
    "    neurips_2023_data = json.load(f)\n",
    "\n",
    "with open(\"data/stopwords.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_2022 = [d.lower() for d in neurips_2022_data.keys()]\n",
    "titles_2023 = [d.lower() for d in neurips_2023_data.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(titles, stopwords, filename):\n",
    "    title_text = ' '.join(titles)\n",
    "    title_text = ' '.join([word for word in title_text.split() if word not in stopwords])\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        max_font_size=100, \n",
    "        min_font_size = 10, \n",
    "        max_words=100, \n",
    "        background_color=\"white\", \n",
    "        width = 533*2, \n",
    "        height = 253*2, \n",
    "        mode=\"RGBA\"\n",
    "        ).generate(title_text)\n",
    "    wordcloud.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, titles in zip([\"2022\", \"2023\"], [titles_2022, titles_2023]):\n",
    "    generate_wordcloud(titles, stopwords, f\"images/wordcloud_{year}.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023 Title Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neurips 2023 Title Wordcloud](images/wordcloud_2023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(neurips_2023_data.keys())\n",
    "\n",
    "abstracts = [neurips_2023_data[key][\"abstract\"] for key in keys]\n",
    "abstracts = [abstract.lower() for abstract in abstracts if abstract!=\"\"]\n",
    "abstract_lengths = [len(abstract.split()) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(abstract_lengths, bins=100, density=True);\n",
    "plt.xlabel(\"Abstract Length (words)\");\n",
    "plt.ylabel(\"Density\");\n",
    "plt.title(\"Abstract Lengths for NeurIPS 2023 Papers\");\n",
    "plt.savefig(\"images/abstract_histogram_2023.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_abstracts = sorted(zip(abstract_lengths, abstracts), key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_abstracts[0][1])\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(f\"Num words {len(sorted_abstracts[0][1].split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ From \"[Re] On the Reproducibility of \\u201cFairCal: Fairness Calibration for Face Verification\\u201d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_abstracts[-1][1])\n",
    "\n",
    "print(f\"Num words {len(sorted_abstracts[-1][1].split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ This is a spotlight poster: \"Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_authors_2022 = [\n",
    "    len(v) for v in neurips_2022_data.values()\n",
    "]\n",
    "\n",
    "num_authors_2023 = [\n",
    "    len(neurips_2023_data[k]['authors']) for k in keys\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "author_counts_2022 = Counter(num_authors_2022)\n",
    "author_counts_2023 = Counter(num_authors_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = sorted(set(author_counts_2022.keys()).union(set(author_counts_2023.keys())))\n",
    "values_2022 = [author_counts_2022.get(k, 0) for k in all_keys]\n",
    "values_2023 = [author_counts_2023.get(k, 0) for k in all_keys]\n",
    "\n",
    "# Set up the bar width and positions\n",
    "bar_width = 0.35\n",
    "r1 = range(len(all_keys))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(r1, values_2022, color='blue', width=bar_width, edgecolor='grey', label='2022')\n",
    "plt.bar(r2, values_2023, color='red', width=bar_width, edgecolor='grey', label='2023')\n",
    "# Add labels and title\n",
    "plt.xlabel('Number of Authors')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.title('Comparison of Number of Authors per Paper for 2022 vs 2023')\n",
    "plt.xticks([r + bar_width / 2 for r in r1], all_keys);\n",
    "\n",
    "# Add a legend\n",
    "plt.legend();\n",
    "plt.savefig(\"images/num_authors_2022_2023.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there were some papers in 2023 with 1 author, when there were none in 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Number of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_authors_2022 = np.mean(num_authors_2022)\n",
    "mean_authors_2023 = np.mean(num_authors_2023)\n",
    "\n",
    "print(f\"Mean number of authors per paper in 2022: {mean_authors_2022}\")\n",
    "print(f\"Mean number of authors per paper in 2023: {mean_authors_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the single author papers, the mean number of authors in 2023 is still higher than in 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper with the Most Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurips_2023_data[keys[int(np.argmax(num_authors_2023))]][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Prolific Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: This assumes that the author names are consistent across papers, and that the author names are unique, which is not always the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors_2022 = set()\n",
    "for v in neurips_2022_data.values():\n",
    "    unique_authors_2022.update(v)\n",
    "\n",
    "unique_authors_2023 = set()\n",
    "for v in neurips_2023_data.values():\n",
    "    unique_authors_2023.update(v[\"authors\"])\n",
    "\n",
    "print(f\"Number of unique authors in 2022: {len(unique_authors_2022)}\")\n",
    "print(f\"Number of unique authors in 2023: {len(unique_authors_2023)}\")\n",
    "\n",
    "authors_counts = {\n",
    "    author_name: 0 for author_name in unique_authors_2023\n",
    "}\n",
    "\n",
    "for v in neurips_2023_data.values():\n",
    "    for author in v[\"authors\"]:\n",
    "        authors_counts[author] += 1\n",
    "\n",
    "authors_counts = sorted(authors_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Most prolific authors in 2023:\")\n",
    "print(\"-----------------------------\")\n",
    "for author, count in authors_counts[:10]:\n",
    "    print(f\"{author}: {count} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Length Comparison between 2022 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_lengths_2022 = [len(title.split()) for title in titles_2022]\n",
    "title_lengths2023 = [len(neurips_2023_data[key][\"title\"].split()) for key in keys]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(title_lengths_2022, bins=100, density=True, label=\"2022\");\n",
    "plt.hist(title_lengths2023, bins=100, density=True, label=\"2023\");\n",
    "plt.xlabel(\"Title Length (words)\");\n",
    "plt.ylabel(\"Density\");\n",
    "plt.legend();\n",
    "plt.savefig(\"images/title_length_histogram_2022_2023.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_title_length_2022 = np.mean(title_lengths_2022)\n",
    "mean_title_length_2023 = np.mean(title_lengths2023)\n",
    "\n",
    "print(f\"Mean title length in 2022: {mean_title_length_2022}\")\n",
    "print(f\"Mean title length in 2023: {mean_title_length_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are trending longer in 2023!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _introduces_acronym(title):\n",
    "    return \":\" in title and len(title.split(\":\")[0].split(\"(\")[0].split()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_acronyms_2022 = sum([_introduces_acronym(title) for title in titles_2022])\n",
    "num_acronyms_2023 = sum([_introduces_acronym(neurips_2023_data[key][\"title\"]) for key in keys])\n",
    "\n",
    "percentage_acronyms_2022 = num_acronyms_2022 / len(titles_2022)\n",
    "percentage_acronyms_2023 = num_acronyms_2023 / len(keys)\n",
    "\n",
    "print(f\"Percentage of titles that introduce acronyms in 2022: {percentage_acronyms_2022}\")\n",
    "print(f\"Percentage of titles that introduce acronyms in 2023: {percentage_acronyms_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the acronyms per paper are trending up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles with LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_titles_with_latex_2022 = sum([\"$\" in title for title in titles_2022])\n",
    "num_titles_with_latex_2023 = sum([\"$\" in neurips_2023_data[key][\"title\"] for key in keys])\n",
    "\n",
    "percentage_titles_with_latex_2022 = num_titles_with_latex_2022 / len(titles_2022)\n",
    "percentage_titles_with_latex_2023 = num_titles_with_latex_2023 / len(keys)\n",
    "\n",
    "print(f\"Percentage of titles that contain LaTeX in 2022: {percentage_titles_with_latex_2022}\")\n",
    "print(f\"Percentage of titles that contain LaTeX in 2023: {percentage_titles_with_latex_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas no titles had LaTeX in 2022, about 1% of titles in 2023 have LaTeX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Analysis of Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [neurips_2023_data[key][\"abstract\"].lower() for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_occurrences_of_words(abstracts, words):\n",
    "    '''\n",
    "    Counts the number of abstracts that contain at least one \n",
    "    of the words in the list of words.\n",
    "    '''\n",
    "    i = 0\n",
    "    for abstract in abstracts:\n",
    "        for word in words:\n",
    "            if word in abstract:\n",
    "                i += 1\n",
    "                break\n",
    "    return i\n",
    "\n",
    "def _print_occurrences_of_words(abstracts, words):\n",
    "    '''\n",
    "    Prints the number of abstracts that contain at least one \n",
    "    of the words in the list of words.\n",
    "    '''\n",
    "    num_occurrences_2023 = _count_occurrences_of_words(abstracts, words)\n",
    "    print(f\"# occurrences of {words[0]} words: {num_occurrences_2023}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BROAD_STROKES_KEYWORDS = (\n",
    "    [\"sampling\"],\n",
    "    [\"differentiable\", \"differentiation\"],\n",
    "    [\"model\"],\n",
    "    [\"data\"],\n",
    "    [\"benchmark\"],\n",
    "    [\"representation\"],\n",
    "    [\"robust\", \"robustness\", \"robustly\"],\n",
    "    [\"learning\", \"learn\"],\n",
    "    [\"foundation\"],\n",
    ")\n",
    "\n",
    "MODALITY_KEYWORDS = (\n",
    "    [\"vision\", \"visual\"],\n",
    "    [\"language\"],\n",
    "    [\"audio\"],\n",
    "    [\"text\"],\n",
    "    [\"video\"],\n",
    "    [\"speech\"],\n",
    "    [\"time series\", \"time-series\", \"temporal\"],\n",
    "    [\"multi-modal\", \"multimodal\"],\n",
    "    [\"cross-modal\", \"crossmodal\"],\n",
    ")\n",
    "\n",
    "TASK_KEYWORDS = (\n",
    "    [\"classification\", \"classify\"],\n",
    "    [\"regression\", \"regress\"],\n",
    "    [\"generation\", \"generate\"],\n",
    "    [\"translation\", \"translate\"],\n",
    "    [\"segmentation\", \"segment\"],\n",
    "    [\"detection\", \"detect\"],\n",
    "    [\"localization\", \"localize\"],\n",
    "    [\"reconstruction\", \"reconstruct\"],\n",
    "    [\"representation\", \"represent\"],\n",
    "    [\"embedding\", \"embed\"],\n",
    "    [\"prediction\", \"predict\"],\n",
    "    [\"synthesis\", \"synthesize\"],\n",
    "    [\"imitation\", \"imitate\"],\n",
    "    [\"plan\"],\n",
    "    [\"control\"],\n",
    "    [\"search\"],\n",
    "    [\"optimization\", \"optimize\"],\n",
    "    [\"adversarial\", \"adversary\", \"adversaries\"],\n",
    "    [\"generative\", \"generation\"],\n",
    "    [\"privacy\", \"private\"],\n",
    "    [\"distill\"],\n",
    "    [\"federated\"],\n",
    "    [\"adapt\"],\n",
    "    [\"transfer\"],\n",
    "    [\"meta\"],\n",
    "    [\"few-shot\", \"few shot\"],\n",
    "    [\"zero-shot\", \"zero shot\"],\n",
    "    [\"fine-tuning\", \"finetuning\"],\n",
    "    [\"reinforcement learning\", \" rl \", \"(rl)\"],\n",
    "    [\"multi-task\", \"multitask\"],\n",
    ")\n",
    "\n",
    "THEORY_KEYWORDS = (\n",
    "    [\"bayes\"],\n",
    "    [\"monte-carlo\", \"monte carlo\"],\n",
    "    [\"gauss\"],\n",
    "    [\"efficient\", \"efficiency\"],\n",
    "    [\"supervised\", \"supervision\"],\n",
    ")\n",
    "\n",
    "MODEL_KEYWORDS = (\n",
    "    [\"clip\", \"contrastive language-image pre-training\"],\n",
    "    [\"nerf\", \"neural radiance fields\"],\n",
    "    [\"transformer\"],\n",
    "    [\"llm\", \"large language model\"],\n",
    "    [\"gpt\"],\n",
    "    [\"gnn\", \"graph neural network\"],\n",
    "    [\"agent\"]\n",
    ")\n",
    "\n",
    "LANGUAGE_KEYWORDS = [\n",
    "    [\"open\"],\n",
    "    [\"vocabulary\"],\n",
    "    [\"prompt\"],\n",
    "    [\"grounding\", \"grounded\"],\n",
    "]\n",
    "\n",
    "DIMENSION_KEYWORDS = [\n",
    "    [\"2d\"],\n",
    "    [\"3d\"],\n",
    "    [\"4d\"],\n",
    "    [\"6d\"],\n",
    "]\n",
    "\n",
    "MULTIMODAL_KEYWORDS = [\n",
    "    [\"vision-language\", \"vision language\"],\n",
    "    [\"text-to-image\", \"text to image\", \"t2i\"],\n",
    "    [\"image-to-text\", \"image to text\", \"i2t\"],\n",
    "    [\"text-to-video\", \"text to video\", \"t2v\"],\n",
    "    [\"captioning\"],\n",
    "    [\"visual question answering\", \"vqa\"],\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------BROAD STROKES--------\")\n",
    "for words in BROAD_STROKES_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------MODALITY--------\")\n",
    "for words in MODALITY_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------TASK--------\")\n",
    "for words in TASK_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------THEORY--------\")\n",
    "for words in THEORY_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------MODEL--------\")\n",
    "for words in MODEL_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------LANGUAGE--------\")\n",
    "for words in LANGUAGE_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------DIMENSION--------\")\n",
    "for words in DIMENSION_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------MULTIMODAL--------\")\n",
    "for words in MULTIMODAL_KEYWORDS:\n",
    "    _print_occurrences_of_words(abstracts, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
